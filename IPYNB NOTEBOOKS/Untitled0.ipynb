{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled0.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMLGSzbru+jAhls2cShkeYJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":[""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EMwX73C1srah","executionInfo":{"status":"ok","timestamp":1645523217450,"user_tz":-330,"elapsed":1038,"user":{"displayName":"azyo asl","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17530650473661026803"}},"outputId":"a3fd241b-b17c-4706-9fed-7f4dd15e6da9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'ASLClean'...\n","remote: Enumerating objects: 11, done.\u001b[K\n","remote: Counting objects: 100% (11/11), done.\u001b[K\n","remote: Compressing objects: 100% (7/7), done.\u001b[K\n","remote: Total 11 (delta 0), reused 8 (delta 0), pack-reused 0\u001b[K\n","Unpacking objects: 100% (11/11), done.\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"oY-FeKho2XvB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# IF file is over 100 mb \n","# #Download git-lfs to Push Files larger than 100MB.\n","# !wget -O git-lfs.tar.gz https://github.com/git-lfs/git-lfs/releases/download/v2.13.2/git-lfs-linux-amd64-v2.13.2.tar.gz\n","# !tar xzf git-lfs.tar.gz\n","# !bash ./install.sh\n","# !git lfs install\n","# %cd <REPO_NAME>\n","# #FILE_NAME is the file with size >100MB and you wants to PUSH to GITHUB\n","# !git lfs track <FILE_NAME>"],"metadata":{"id":"-iS-S76Y2XqI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git init ASLClean\n","!git add collection.py\n","!git commit -m 'added collection utility '  # commit in Colab\n","!git push https://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@github.com/cskaa/ASLClean                                  # push to github"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dRVDgn8M1ITT","executionInfo":{"status":"ok","timestamp":1645524221857,"user_tz":-330,"elapsed":1028,"user":{"displayName":"azyo asl","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17530650473661026803"}},"outputId":"1f933bb6-9cc3-4f83-b7e5-8468ff31536f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Reinitialized existing Git repository in /content/ASLClean/.git/\n","fatal: not a git repository (or any of the parent directories): .git\n","fatal: not a git repository (or any of the parent directories): .git\n","fatal: not a git repository (or any of the parent directories): .git\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cOzMgoPLCLoR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645437093998,"user_tz":-330,"elapsed":10069,"user":{"displayName":"azyo asl","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17530650473661026803"}},"outputId":"5234ef4f-6ab3-4fe3-d6b8-21b63fadcd3d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting mediapipe\n","  Downloading mediapipe-0.8.9.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32.7 MB)\n","\u001b[K     |████████████████████████████████| 32.7 MB 248 kB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mediapipe) (1.21.5)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from mediapipe) (3.2.2)\n","Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.7/dist-packages (from mediapipe) (4.1.2.30)\n","Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.7/dist-packages (from mediapipe) (21.4.0)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from mediapipe) (1.0.0)\n","Requirement already satisfied: protobuf>=3.11.4 in /usr/local/lib/python3.7/dist-packages (from mediapipe) (3.17.3)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.11.4->mediapipe) (1.15.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (0.11.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (1.3.2)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (2.8.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (3.0.7)\n","Installing collected packages: mediapipe\n","Successfully installed mediapipe-0.8.9.1\n"]}],"source":["!pip install mediapipe\n","from tensorflow.keras.models import load_model\n","from cv2 import VideoCapture, waitKey, imshow, cvtColor, flip, COLOR_BGR2RGB, COLOR_RGB2BGR, putText, FONT_HERSHEY_SIMPLEX, resize, rectangle, line\n","import mediapipe as mp\n","import cv2\n","from numpy import asscalar, ndarray, array, expand_dims\n","from sklearn.metrics import confusion_matrix\n","import pandas as pd\n","from pathlib import Path\n","from csv import writer\n","from numpy import ndarray, array, zeros, concatenate\n","from shutil import move"]},{"cell_type":"code","source":[""],"metadata":{"id":"LDU6nd9H1HTF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Flow_csv():\n","    '''\n","    This class dynamically loads data to not overflow memeory\n","    \n","    init folder format:\n","    example: \n","        cats -> 100 csv files\n","        dogs -> 100 csv files\n","        etcs -> 100 csv files\n","\n","    '''\n","    def __init__(self, path_to_folder) -> None:\n","        # set meta folder path\n","        self.path_to_folder = path_to_folder\n","        # get class list\n","        self.classes = self.get_class_list(path_to_folder)\n","\n","        pass\n","\n","    def get_class_list(self, path_to_folder) -> list:\n","        '''\n","        This function returns a list of classes in the folder\n","        '''\n","        class_list = []\n","        for folder in os.listdir(path_to_folder):\n","            class_list.append(folder)\n","        return class_list\n","\n","    def get_path_of_all_csv(self, class_name) -> list:\n","        '''\n","        This function returns a list of csv files in the folder\n","        '''\n","        path_list = []\n","        for file in os.listdir(self.path_to_folder + class_name):\n","            path_list.append(self.path_to_folder + class_name + '/' + file)\n","        return path_list"],"metadata":{"id":"MJLTmeUfDSO2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install vidaug\n","import random\n","from pathlib import Path\n","from shutil import rmtree\n","from vidaug import augmentors \n","\n","from ASLClean.modules.dataloader import Flow_csv\n","from ASLClean.ASL_UTILS.collection import Writer\n","\n","\n","class augmenter():\n","    '''\n","    This class randomly augments mvi video files \n","    init folder format:\n","    example: \n","        cats -> 100 mvi files\n","        dogs -> 100 mvi files\n","        etcs -> 100 mvi files\n","\n","    '''\n","    def __init__(self, path_to_source_folder, path_to_destination_folder, augmentation_per_class) -> None:\n","        # set meta folder path\n","        self.path_to_source_folder = path_to_source_folder\n","        self.path_to_destination_folder = path_to_destination_folder\n","        self.augmentation_per_class = augmentation_per_class\n","        pass\n","\n","\n","\n","    def mvi_augmenter(self, path_to_source_folder, path_to_destination_folder, augmentation_per_class) -> None:\n","        '''\n","        This function augments the mvi files and store them in the destination folder\n","        '''\n","        aug_fail = 0    #check for failed augmentations\n","        destination = Path(path_to_destination_folder)\n","        class_list = Flow_csv.get_class_list(path_to_source_folder)\n","        path_list = Flow_csv.get_path_of_all_csv(class_list)\n","              \n","        random.seed(1)\n","        counter_class = len(class_list)\n","        for i in range(len(class_list)):\n","            vid_count = augmentation_per_class\n","            category = class_list[i]\n","            current_ds = path_list[i]                       #current_ds is the path_list of class[i]\n","            make_dir = Path(path_to_destination_folder)     #making destination folder for class[i]\n","            make_dir = make_dir / category\n","            if make_dir.exists(): rmtree(make_dir)\n","            make_dir.mkdir()\n","            for _ in range(augmentation_per_class):\n","                cap = cv2.VideoCapture(str(random.choice(current_ds)))\n","                video_frames = []\n","                while True:\n","                    status, frame = cap.read()\n","                    if not status: break\n","                    video_frames.append(frame)\n","                video_shape = video_frames[0].shape\n","                seq = augmentors.Sequential([augmentors.RandomTranslate(int(video_shape[1]/4),50), augmentors.RandomShear(0.07,0.07)])\n","                video_aug = seq(video_frames)\n","                aug_vid_name = destination/category/f\"{random.randint(100000,999999)}.avi\"\n","                if len(video_aug) == 50:                    #checking if the augmented video is actually of 50 frames\n","                    for aug_frame in video_aug:\n","                        Writer.write_as_video(str(aug_vid_name), aug_frame)\n","                else:\n","                    aug_fail +=1\n","                print(\"class:\",counter_class,\"||\",\"video no:\",vid_count,\"/\",augmentation_per_class,\"||\",\"filename:\",aug_vid_name)\n","                vid_count -=1\n","            counter_class -=1\n","        print(\"Failed augmented videos :\" , str(aug_fail))\n","\n"],"metadata":{"id":"WCLRqyOuAi3H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install mediapipe\n","import mediapipe\n","from cv2 import VideoCapture\n","import os\n","\n","from ASLClean.modules.collection import ImageHandler, Writer\n","from ASLClean.modules.extractor import MediapipeExtractor\n","class CSV_maker():\n","    '''\n","    This class reads mvi files, extracts mediapipe landmark and writes them in a csv file\n","    init folder format:\n","    \n","    init folder format:\n","    example: \n","        cats -> 100 mvi files\n","        dogs -> 100 mvi files\n","        etcs -> 100 mvi files\n","\n","    '''\n","\n","    def __init__(self, path_to_source_folder ,path_to_destination_folder ,mode) -> None:\n","        # set meta folder path\n","        self.path_to_source_folder = path_to_source_folder\n","        self.path_to_destination_folder = path_to_destination_folder\n","        self.mode = mode\n","        pass\n","    \n","    \n","    \n","    def extract_landamrks(self, path_to_source_folder ,path_to_destination_folder ,mode) -> None:\n","        '''\n","        This function returns a list of csv files in the folder\n","\n","        '''\n","        mp_drawing = mediapipe.solutions.drawing_utils\n","        mp_holistic = mediapipe.solutions.holistic\n","\n","        destination = Path(path_to_destination_folder)\n","        class_list = Flow_csv.get_class_list(path_to_source_folder)\n","        path_list = Flow_csv.get_path_of_all_csv(class_list)  \n","        counter_class = len(class_list)\n","        with mp_holistic.Holistic(\n","            min_detection_confidence = 0.5,\n","            min_tracking_confidence = 0.5\n","        ) as holistic:\n","            for i in range(counter_class):\n","                category = class_list[i]\n","                current_ds = path_list[i]                       #current_ds is the path_list of class[i]\n","                out_dir = Path(path_to_destination_folder + '/' + category)\n","                if out_dir.exists(): rmtree(out_dir)            #WARNING! MAKE SURE out_directory IS EMPTY OR EVERYTHING WILL BE DELETED\n","                out_dir.mkdir()\n","                progress = 0\n","                for j in range(len(current_ds)):\n","                    cap = VideoCapture(str(current_ds[j]))\n","                    frame_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) \n","                    out_prefix = category \n","                    print( \"Progress :\" + str(progress) + \" ||\" + str(len(current_ds)) ) \n","                    progress+=1   \n","                    video_name = os.path.basename(os.path.normpath(str(current_ds[j])))\n","                    video_name = str(os.path.splitext(video_name)[0])\n","                    print( \"New Video loaded\" + \"\\t Category : \" + category + \"\\t Name : \" + video_name + \"\\n\"   )  \n","                    while cap.isOpened() and frame_length!=0:  \n","                        image: ndarray\n","                        success, image = cap.read()\n","\n","                        if not success: continue\n","                        image = cvtColor(image, COLOR_BGR2RGB)\n","\n","                        image.flags.writeable = False\n","                        results = holistic.process(image)\n","\n","                        image = cvtColor(image, COLOR_RGB2BGR)\n","                        image = ImageHandler.draw_results(image, results)\n","\n","                        landmarks = MediapipeExtractor.extract_landmarks(results)\n","\n","                        Writer.write_to_csv(landmarks, destination / f'{out_prefix}_{video_name}.csv')\n","                        frame_length-=1                        \n","\n","\n"],"metadata":{"id":"Fe69pOXNyFKA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","print(os.path.basename(os.path.normpath('/content/sample_data/mnist_test.csv')))"],"metadata":{"id":"8OJoZwDcDMv-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645519608342,"user_tz":-330,"elapsed":10,"user":{"displayName":"azyo asl","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17530650473661026803"}},"outputId":"53b368c9-f1cd-40c0-8a3e-94c8d9351f1d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["mnist_test.csv\n"]}]},{"cell_type":"code","source":["import os\n","class_list = []\n","for folder in os.listdir('/content/sample_data'):\n","    class_list.append(folder)\n","print(class_list)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tSKmSvfuDMq0","executionInfo":{"status":"ok","timestamp":1645443108944,"user_tz":-330,"elapsed":374,"user":{"displayName":"azyo asl","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17530650473661026803"}},"outputId":"8b78447e-1652-443f-a437-d99f846ec9ad"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['anscombe.json', 'README.md', 'california_housing_train.csv', 'mnist_train_small.csv', 'mnist_test.csv', 'california_housing_test.csv']\n"]}]},{"cell_type":"code","source":["path_list = []\n","for class_name in class_list:\n","    for file in os.listdir('/content/sample_data/' + class_name):\n","        path_list.append('/content/sample_data/' + class_name + '/' + file)\n","    video_name = os.path.basename(os.path.normpath(str(path_list[1][1])))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":234},"id":"0eYtYImCCtdt","executionInfo":{"status":"error","timestamp":1645443189741,"user_tz":-330,"elapsed":395,"user":{"displayName":"azyo asl","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17530650473661026803"}},"outputId":"c70b1f50-7546-4ebd-92cd-4ac4d7b6ec19"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NotADirectoryError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-38a7dbdd2a11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpath_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mclass_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclass_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/sample_data/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mpath_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/sample_data/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mclass_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvideo_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: '/content/sample_data/anscombe.json'"]}]},{"cell_type":"code","source":["from pandas import read_csv, get_dummies, Series, concat\n","from numpy import array, random, eye, argmax\n","import pickle\n","from tensorflow.keras.models import Sequential, load_model\n","from tensorflow.keras.layers import LSTM, Dense\n","from tensorflow.keras.callbacks import TensorBoard, Callback, ModelCheckpoint, CSVLogger\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.python.keras.layers.core import Dropout\n","from sklearn.metrics import  confusion_matrix, ConfusionMatrixDisplay\n","import matplotlib.pyplot as plt\n","from os.path import split\n","from shutil import rmtree\n","import random"],"metadata":{"id":"nbpPu2gMBi2v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install tensorflow==2.4.0      \n","MAIN FILE ABSOLUTELY DONOT MISS THIS PART"],"metadata":{"id":"qAPBMmV6Bi0X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install py7zr\n","from re import findall\n","import py7zr\n","from pathlib import Path\n","from random import shuffle\n","import pickle\n","from pandas import DataFrame, get_dummies, read_csv,\n","from numpy import array\n","import random\n","from csv import writer\n","\n","\n","\n","\n","class ASL_train():\n","    '''\n","    This class reads csv files classwise, splits them in a 0.8 training data and 0.2 validation data and trains the model\n","    init folder format:\n","    \n","    init folder format:\n","    example:\n","        csv -> cats -> 100 csv files\n","            -> dogs -> 100 csv files\n","            -> etcs -> 100 csv files\n","    '''\n","\n","    def __init__(self, path_to_source_folder ,path_to_destination_folder ,ACCURACY_THRESHOLD = 0.95, resume=False) -> None:\n","        # set meta folder path\n","\n","        #Extract if zip/7z or continue with source folder\n","        if findall('.7z', str(path_to_source_folder)):\n","            with py7zr.SevenZipFile('/content/drive/MyDrive/ASL_dataset/augmented_csv.7z', mode='r') as z:\n","                z.extractall('/content/ASLdataset')\n","            self.path_to_source_folder = Path('/content/ASLdataset')\n","        elif findall('.zip', str(path_to_source_folder)):\n","            !unzip $path_to_source_folder -d /content/ASLdataset\n","            self.path_to_source_folder = Path('/content/ASLdataset')\n","        else:\n","            self.path_to_source_folder = Path(path_to_source_folder)\n","        \n","        self.model_out = Path(path_to_destination_folder)\n","        self.final_model = Path(path_to_destination_folder + '/Best_model' )\n","        self.ACCURACY_THRESHOLD = ACCURACY_THRESHOLD\n","        self.resume = resume\n","\n","        pass\n","\n","    def custom_generator(csv_list, batch_size, classes):\n","        i = 0\n","        while True:\n","            batch = {'csv': [], 'labels': []}\n","            fail_check = 0\n","            while fail_check < batch_size:\n","                if i == (int(len(csv_list))):\n","                    shuffle(csv_list)\n","                    i=0\n","\n","                # Read data from csv using the name from csv_list\n","                csv_path = csv_list[i]\n","                label = split(split(csv_path)[0])[1]\n","\n","                d = read_csv(str(\"/\" + str(csv_path)), header=None ).values\n","                if d.shape == (50,1629):\n","                    random_number = random.randint(1,30)\n","                    batch['csv'].append(d[random_number:random_number+20])\n","                    batch['labels'].append(label)\n","                    fail_check+=1\n","                i += 1\n","            batch['csv'] = array(batch['csv'])\n","            # Convert labels to categorical values\n","            df = DataFrame({'cat': classes})\n","            dummies = get_dummies(df,prefix='', prefix_sep='')\n","            batch['labels'] = dummies.T.reindex(batch['labels']).fillna(0).values\n","\n","            yield tuple([batch['csv'], batch['labels']])\n","\n","    def extract_highest_number(file_list):\n","        s = re.findall(\"\\d+$\",file_list)\n","        return (int(s[0]) if s else -1,file_list)\n","\n","\n","    def train(self, path_to_source_folder ,model_out ,final_model, ACCURACY_THRESHOLD, resume ) -> None:\n","                \n","        x_valid =[]\n","        y_valid = []\n","        if resume == False:\n","            all_source_ds = []\n","            all_source_ds_flat = []\n","            classes = list(x.name for x in path_to_source_folder.glob('*'))\n","            for category in classes:\n","                cat_dir = list((path_to_source_folder/category).glob(\"*\"))\n","                all_source_ds.append(cat_dir)\n","            all_source_ds_flat = [item for sublist in all_source_ds for item in sublist]\n","            shuffle(all_source_ds_flat)\n","            valid_csv_list = all_source_ds_flat[:int(len(all_source_ds_flat)*0.2)]\n","            train_csv_list = all_source_ds_flat[int(len(all_source_ds_flat)*0.2):]\n","            with open(str(model_out)+'/finaltrain_csv_list.pkle','wb') as xyz:\n","                pickle.dump(train_csv_list,xyz)\n","            with open(str(model_out)+'/finalvalid_csv_list.pkle','wb') as xyz:\n","                pickle.dump(valid_csv_list,xyz)\n","        else:\n","            with open(str(model_out)+'/finaltrain_csv_list.pkle','rb') as xyz:\n","                train_csv_list = pickle.load(xyz)\n","            with open(str(model_out)+'/finalvalid_csv_list.pkle','rb') as xyz:\n","                valid_csv_list = pickle.load(xyz)\n","            shuffle(train_csv_list)\n","            shuffle(valid_csv_list)\n","        csv_name = model_out / \"logs.csv\"\n","        if not csv_name.exists():\n","            with open(csv_name, 'w') as csvfile:\n","                csvwriter = csv.writer(csvfile) \n","                csvwriter.writerow(['count','accuracy','val_accuracy','loss','val_loss'])\n","        \n","\n","\n","\n","\n","class stop_(Callback): \n","    def __init__(self, logs_file) -> None:\n","        self.logs_file = logs_file\n","\n","        pass\n","\n","\n","    def on_epoch_end(self, epoch, logs={},logs_file):\n","        model_files = list(x.name for x in source_directory.glob('*'))\n","        start_count = max(model_files,key=extract_highest_number)\n","        start_count = re.findall(\"\\d+$\",start_count)\n","        epoch_count = epoch + int(start_count[0]) + 1\n","        model.save(Path(str(model_out) + \"/model\" + str(epoch_count)))\n","        # CSVLogger(str(model_out) + \"/model_history_log.csv\", append=True)     #Doesnt work for some reason\n","        with open(csv_name, 'a+') as csvfile:\n","            csvwriter = writer(csvfile) \n","            csvwriter.writerow([epoch_count,logs.get('accuracy'),logs.get('val_accuracy'),logs.get('loss'),logs.get('val_loss')])\n","        if(logs.get('accuracy') > ACCURACY_THRESHOLD):\n","                print(\"\\nReached %2.2f%% accuracy, so stopping training!!\" %(ACCURACY_THRESHOLD*100))   \n","                self.model.stop_training = True\n","\n","        \n","\n","\n"],"metadata":{"id":"n1Ubpe3DBixs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["path_to_destination_folder='/content/ASLdataset'\n","model_name = Path(path_to_destination_folder + '/Best_model' )\n","print (model_name)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R5i6bAQ6BisN","executionInfo":{"status":"ok","timestamp":1647007222478,"user_tz":-330,"elapsed":603,"user":{"displayName":"azyo asl","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17530650473661026803"}},"outputId":"416f4cd3-02fd-4be5-86be-cf4ca3baa6d3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/ASLdataset/Best_model\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"nwqqES6JBipO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[""],"metadata":{"id":"-sN_wnxjAivo"}}]}