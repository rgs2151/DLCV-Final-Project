{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"colab_test_WIP.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1fiz7JTu4xQ6N6cLmRSDQYUotVs1EsYnC","authorship_tag":"ABX9TyOgh3HVgR6cfyE+5oHbvmoh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u-lr6lgnFnNs","executionInfo":{"status":"ok","timestamp":1643871341590,"user_tz":-330,"elapsed":17021,"user":{"displayName":"azyo asl","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17530650473661026803"}},"outputId":"4c488996-0d6a-4ce5-d595-ec0ba4326467"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting mediapipe\n","  Downloading mediapipe-0.8.9.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32.7 MB)\n","\u001b[K     |████████████████████████████████| 32.7 MB 163 kB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mediapipe) (1.19.5)\n","Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.7/dist-packages (from mediapipe) (4.1.2.30)\n","Requirement already satisfied: protobuf>=3.11.4 in /usr/local/lib/python3.7/dist-packages (from mediapipe) (3.17.3)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from mediapipe) (3.2.2)\n","Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.7/dist-packages (from mediapipe) (21.4.0)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from mediapipe) (1.0.0)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.11.4->mediapipe) (1.15.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (0.11.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (3.0.7)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (2.8.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (1.3.2)\n","Installing collected packages: mediapipe\n","Successfully installed mediapipe-0.8.9.1\n","Thu Feb  3 06:55:41 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   33C    P8    28W / 149W |      0MiB / 11441MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["'''\n","CLEARING THE ARRAY AFTER EACH PREDICTION\n","'''\n","!pip install mediapipe\n","from cv2 import VideoCapture, waitKey, imshow, cvtColor, flip, COLOR_BGR2RGB, COLOR_RGB2BGR, putText, FONT_HERSHEY_SIMPLEX, resize, rectangle, line\n","from numpy import ndarray, array, argmax, expand_dims, fromstring, zeros, concatenate\n","from pathlib import Path\n","from shutil import rmtree, move\n","from tensorflow.keras.models import load_model\n","from collections import deque\n","import time\n","import mediapipe as mp\n","from csv import writer\n","!nvidia-smi"]},{"cell_type":"code","source":["class MediapipeExtractor:\n","    def extract_landmarks(self, results) -> ndarray:\n","        result_landmarks = {\n","            'face': {\n","                'landmark': results.face_landmarks,\n","                'shape': (468,)\n","            },\n","            'left hand': {\n","                'landmark': results.left_hand_landmarks,\n","                'shape': (21,)\n","            },\n","            'right hand': {\n","                'landmark': results.right_hand_landmarks,\n","                'shape': (21,)\n","            },\n","            'pose': {\n","                'landmark': results.pose_landmarks,\n","                'shape': (33,)\n","            }\n","        }\n","\n","        result_all = []\n","        result_temp = []\n","        for key, result in result_landmarks.items():\n","            if result['landmark']:\n","                result_temp = array([array([l.x, l.y, l.z]) for l in result['landmark'].landmark])\n","                result_temp = result_temp.flatten()\n","            else:\n","                result_temp = zeros((result['shape'][0] * 3, ))\n","\n","            assert(result_temp.shape == (result['shape'][0] * 3, ))\n","            result_all.append(result_temp)\n","            \n","        shape = 0\n","        result_final = array([])\n","        for result in result_all:\n","            shape += result.shape[0]\n","            result_final = concatenate((result_final, result), axis=0)\n","\n","        assert(result_final.shape == (shape,))\n","\n","        return result_final"],"metadata":{"id":"M0gG1OlnF4TE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ImageHandler:\n","    def draw_results(self, image, results):\n","        image.flags.writeable = True\n","\n","        # mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS, landmark_drawing_spec=None, connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_contours_style())\n","        mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n","        mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n","        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n","\n","        return image"],"metadata":{"id":"Omdw3mwlF4c_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mp_drawing = mp.solutions.drawing_utils\n","mp_holistic = mp.solutions.holistic\n","cap = VideoCapture(take_photo())\n","me = MediapipeExtractor()\n","im = ImageHandler()\n","\n","# out_dir = Path('test')\n","# if out_dir.exists(): rmtree(out_dir)\n","# out_dir.mkdir()"],"metadata":{"id":"cSpkMDzGF4i-","colab":{"base_uri":"https://localhost:8080/","height":234},"executionInfo":{"status":"error","timestamp":1643871372054,"user_tz":-330,"elapsed":5,"user":{"displayName":"azyo asl","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17530650473661026803"}},"outputId":"fb30bad9-775a-4ea8-c8ae-f9addbebdfb8"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/javascript":["\n","    async function takePhoto(quality) {\n","      const div = document.createElement('div');\n","      const capture = document.createElement('button');\n","      capture.textContent = 'Capture';\n","      div.appendChild(capture);\n","\n","      const video = document.createElement('video');\n","      video.style.display = 'block';\n","      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n","\n","      document.body.appendChild(div);\n","      div.appendChild(video);\n","      video.srcObject = stream;\n","      await video.play();\n","\n","      // Resize the output to fit the video element.\n","      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n","\n","      // Wait for Capture to be clicked.\n","      await new Promise((resolve) => capture.onclick = resolve);\n","\n","      const canvas = document.createElement('canvas');\n","      canvas.width = video.videoWidth;\n","      canvas.height = video.videoHeight;\n","      canvas.getContext('2d').drawImage(video, 0, 0);\n","      stream.getVideoTracks()[0].stop();\n","      div.remove();\n","      return canvas.toDataURL('image/jpeg', quality);\n","    }\n","    "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{}},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-9436eb42f399>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmp_drawing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolutions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrawing_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmp_holistic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolutions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mholistic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVideoCapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtake_photo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mme\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMediapipeExtractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: an integer is required (got type Javascript)"]}]},{"cell_type":"code","source":["QR = []\n","QR_limit = 50\n","QR_status = 0\n","\n","#frame rate\n","prev_frame_time = 0\n","new_frame_time = 0\n","avgfps = 0\n","\n","# model = r\"Z:\\jupyternotebook\\ASL-main\\ASL-main\\test\\Model\\ADB_9_95\"\n","# model = 'Model_new/ADB_9_95'\n","model = load_model('/content/drive/MyDrive/Model_new/ADB_9_95')\n","\n","z=0\n","DQ = deque(maxlen=1)\n","Q = []\n","Q_limit = 70\n","Q_status = 0\n","klasses = ['Address', 'Movie', 'Name', 'Phone', 'Play', 'Please','Work','Your']\n","\n","new_result = \"\"\n","new_result_1 = \"\"\n","text1 = ''\n","results_forever=\"\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YPfSE4sTF4nM","executionInfo":{"status":"ok","timestamp":1643869857950,"user_tz":-330,"elapsed":8705,"user":{"displayName":"azyo asl","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17530650473661026803"}},"outputId":"0931431b-a752-4624-ad41-595dd1df2036"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n","WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"]}]},{"cell_type":"code","source":["width = 640\n","height = 480\n","\n","width_from_center = 175\n","height_from_center_top = 200\n","height_from_center_bottom = int(height / 2)\n","\n","point1 = (int((width/2) - width_from_center), int((height/2) - height_from_center_top))\n","point2 = (int((width/2) + width_from_center), int((height/2) + height_from_center_bottom))\n","\n","lheight = int((height/2) - 10)\n","lpoint1 = (point1[0], lheight)\n","lpoint2 = (point2[0], lheight)\n","\n","fwidth = 100\n","fpoint1 = (int(width/2 - fwidth), point1[1])\n","fpoint2 = ((int(width/2 + fwidth), lpoint1[1]))\n"],"metadata":{"id":"DVzyDvhUF4rh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with mp_holistic.Holistic(\n","    min_detection_confidence = 0.5,\n","    min_tracking_confidence = 0.5\n",") as holistic:\n","\n","    while cap.isOpened():\n","        \n","        image: ndarray\n","        success, image = cap.read()\n","\n","        if not success: continue\n","\n","        image = flip(image, 1)\n","\n","        original = image.copy()\n","\n","        image = cvtColor(image, COLOR_BGR2RGB)\n","\n","        image.flags.writeable = False\n","        results = holistic.process(image)\n","\n","        image = cvtColor(image, COLOR_RGB2BGR)\n","        image = im.draw_results(image, results)\n","\n","        landmarks = me.extract_landmarks(results)\n","\n","\n","        if QR_status == QR_limit:\n","\n","            QR_vid = expand_dims(array(QR), 0)\n","            results = model.predict(QR_vid)\n","\n","            # DQ.append(results[0])\n","            # results = array(DQ).mean(axis=0)\n","            results = array(results).mean(axis=0)\n","            predicted_kls = klasses[results.argmax(axis=0)]\n","            results_forever = results\n","\n","            text1 = f\"{predicted_kls}\"\n","            QR.pop(0)\n","            QR.append(landmarks)\n","            z +=1\n","        else:\n","            QR.append(landmarks)\n","            QR_status += 1\n","            text2 = f\"{'|'*QR_status}\"\n","        \n","        image = resize(image, (1280, 1024))\n","        image = putText(image, str(text1), (30, image.shape[0] - 90), FONT_HERSHEY_SIMPLEX, 2, (11,209,252), 3)\n","        image = putText(image, text2, (30, image.shape[0] - 30), FONT_HERSHEY_SIMPLEX, 2, (11,209,252), 3)\n","        # new_result = (sorted(zip(klasses, results_forever), key=lambda x: x[1], reverse=True))\n","        z=0\n","        for x in sorted(zip(klasses, results_forever)):\n","            if z < 4:\n","                new_result = new_result + \" \" + str(x[0]) + \" \" + str(x[1]) + \" \"\n","                z +=1\n","            else :\n","                new_result_1 = new_result_1 + \" \" + str(x[0]) + \" \" + str(x[1]) + \" \"\n","\n","        try:\n","            image = putText(image, str(new_result), (30, image.shape[0] - 980), FONT_HERSHEY_SIMPLEX, 0.7, (255,174,0), 2)\n","            image = putText(image, str(new_result_1), (30, image.shape[0] - 920), FONT_HERSHEY_SIMPLEX, 0.7, (255,174,0), 2)\n","            new_result = \"\"\n","            new_result_1 = \"\"\n","            \n","        except Exception as err:\n","            # pass\n","            # image = putText(image, str(results), (30, image.shape[0] - 200), FONT_HERSHEY_SIMPLEX, 1, (255,174,0), 2)\n","            print(\"hello\")\n","        #fps counter\n","\n","        new_frame_time = time.time()\n","        fps = 1/(new_frame_time-prev_frame_time)\n","        prev_frame_time = new_frame_time\n","        fps = int(fps)\n","        avgfps=0.9*float(avgfps)+0.1*float(fps)\n","        original_text = str(original.shape) + \"  FPS: \" + str(int(avgfps))\n","        \n","        original = putText(original, original_text, (30, 30), FONT_HERSHEY_SIMPLEX, 1, (11,209,252), 2)\n","        original = rectangle(original, point1, point2, (211, 66, 242), 2)\n","        original = line(original, lpoint1, lpoint2, (211, 66, 242), 2)\n","        original = rectangle(original, fpoint1, fpoint2, (92, 206, 17), 2)\n","            \n","        imshow('out', image)\n","        imshow('original', original)\n","\n","        if waitKey(5) & 0xFF == 27: break # 27 is escape!\n","\n","    cap.release()"],"metadata":{"id":"_j9qAwiYHpfs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"zggiaB7KYrhn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from IPython.display import display, Javascript\n","from google.colab.output import eval_js\n","from base64 import b64decode\n","\n","def take_photo(filename='photo.jpg', quality=0.8):\n","  js = Javascript('''\n","    async function takePhoto(quality) {\n","      const div = document.createElement('div');\n","      const capture = document.createElement('button');\n","      capture.textContent = 'Capture';\n","      div.appendChild(capture);\n","\n","      const video = document.createElement('video');\n","      video.style.display = 'block';\n","      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n","\n","      document.body.appendChild(div);\n","      div.appendChild(video);\n","      video.srcObject = stream;\n","      await video.play();\n","\n","      // Resize the output to fit the video element.\n","      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n","\n","      // Wait for Capture to be clicked.\n","      await new Promise((resolve) => capture.onclick = resolve);\n","\n","      const canvas = document.createElement('canvas');\n","      canvas.width = video.videoWidth;\n","      canvas.height = video.videoHeight;\n","      canvas.getContext('2d').drawImage(video, 0, 0);\n","      stream.getVideoTracks()[0].stop();\n","      div.remove();\n","      return canvas.toDataURL('image/jpeg', quality);\n","    }\n","    ''')\n","  display(js)\n","  data = eval_js('takePhoto({})'.format(quality))\n","  binary = b64decode(data.split(',')[1])\n"," \n","  return filename"],"metadata":{"id":"hClWLMoZqc5q","executionInfo":{"status":"ok","timestamp":1643876773606,"user_tz":-330,"elapsed":399,"user":{"displayName":"azyo asl","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17530650473661026803"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["from IPython.display import Image\n","while True:\n","    take_photo()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":519},"id":"0dbQ7xWxqc5r","outputId":"6f52599b-935e-4c18-c4dc-55aaac65995d"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/javascript":["\n","    async function takePhoto(quality) {\n","      const div = document.createElement('div');\n","      const capture = document.createElement('button');\n","      capture.textContent = 'Capture';\n","      div.appendChild(capture);\n","\n","      const video = document.createElement('video');\n","      video.style.display = 'block';\n","      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n","\n","      document.body.appendChild(div);\n","      div.appendChild(video);\n","      video.srcObject = stream;\n","      await video.play();\n","\n","      // Resize the output to fit the video element.\n","      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n","\n","      // Wait for Capture to be clicked.\n","      await new Promise((resolve) => capture.onclick = resolve);\n","\n","      const canvas = document.createElement('canvas');\n","      canvas.width = video.videoWidth;\n","      canvas.height = video.videoHeight;\n","      canvas.getContext('2d').drawImage(video, 0, 0);\n","      stream.getVideoTracks()[0].stop();\n","      div.remove();\n","      return canvas.toDataURL('image/jpeg', quality);\n","    }\n","    "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{}},{"output_type":"display_data","data":{"application/javascript":["\n","    async function takePhoto(quality) {\n","      const div = document.createElement('div');\n","      const capture = document.createElement('button');\n","      capture.textContent = 'Capture';\n","      div.appendChild(capture);\n","\n","      const video = document.createElement('video');\n","      video.style.display = 'block';\n","      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n","\n","      document.body.appendChild(div);\n","      div.appendChild(video);\n","      video.srcObject = stream;\n","      await video.play();\n","\n","      // Resize the output to fit the video element.\n","      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n","\n","      // Wait for Capture to be clicked.\n","      await new Promise((resolve) => capture.onclick = resolve);\n","\n","      const canvas = document.createElement('canvas');\n","      canvas.width = video.videoWidth;\n","      canvas.height = video.videoHeight;\n","      canvas.getContext('2d').drawImage(video, 0, 0);\n","      stream.getVideoTracks()[0].stop();\n","      div.remove();\n","      return canvas.toDataURL('image/jpeg', quality);\n","    }\n","    "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{}}]}]}