{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled4.ipynb","provenance":[],"mount_file_id":"1fUx_7-BpWRWCjE6EO2dNi7peDOdmdArA","authorship_tag":"ABX9TyOPGFgF06LgZVdKCQqVQlDX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"QPDxDuWmE-gH"},"outputs":[],"source":["import os\n","import numpy as np\n","import torch\n","import torch.utils.data\n","from PIL import Image\n","def txtToArray(path):\n","    f = open(path)\n","    tokens=f.read().split()\n","    tokens = [i for i in tokens if i != '0' and i!='hand']\n","    for i in range(0,len(tokens)): tokens[i]=tokens[i].split(',')\n","    A=np.array(tokens, dtype=np.int64)\n","    return(A)\n","class HandsDataset(torch.utils.data.Dataset):\n","    def __init__(self,root,transforms=None):\n","        self.root = root\n","        self.transforms = transforms\n","        self.imgs = list(os.listdir(os.path.join(root,\"DATA_IMAGES\")))\n","        self.masks = list(os.listdir(os.path.join(root,\"DATA_MASKS\")))\n","        self.boxes = list(os.listdir(os.path.join(root,\"DATA_BOXES\")))\n","    def __getitem__(self,idx):\n","        i = 1+int(idx/100)                                               #i indicates which video\n","        j = 1+(idx%100)                                                  #j indicates which frame within given video\n","        imgStr = \"Image\"+str(i)+\"_\"+str(j)+\".jpg\"\n","        maskStr = \"Mask\"+str(i)+\"_\"+str(j)+\"_\"\n","        boxStr = \"Box\"+str(i)+\"_\"+str(j)+\".txt\"\n","        img_path = os.path.join(self.root, \"DATA_IMAGES\",imgStr)\n","        box_path = os.path.join(self.root,\"DATA_BOXES\",boxStr)\n","        box_array = txtToArray(box_path)\n","        boxes = []\n","        masks = []\n","        for boxid in range(4):  #we go through each bounding box and fetch its corresponding mask image\n","            if box_array[boxid,2] != 0 : #we have bounding box \n","                xmin = box_array[boxid,0]\n","                ymax = box_array[boxid,1]\n","                xmax = xmin+box_array[boxid,2]\n","                ymin = ymax - box_array[boxid,3]\n","                boxes.append([xmin,ymin,xmax,ymax])\n","                #get mask path and add to masks array\n","                maskStrTemp = maskStr +str(boxid)+\".jpg\"\n","                mask_path = os.path.join(self.root,\"DATA_MASKS\",maskStrTemp)\n","                mask = Image.open(mask_path)\n","                mask = np.array(mask)\n","                for i in range(mask.shape[0]):                 #turns mask into a binary (black and white) image\n","                    for j in range(mask.shape[1]):\n","                        if(mask[i,j]!= 0):\n","                            mask[i,j] = 1\n","                masks.append(mask)\n","        img = Image.open(img_path).convert(\"RGB\")\n","        num_objs = len(boxes)\n","        if num_objs == 0:                               \n","            idx = (idx+1)%4800\n","            return self.__getitem__(idx)\n","        boxes = torch.as_tensor(boxes,dtype=torch.float32)\n","        if len(boxes) == 0:\n","            area = torch.as_tensor([0],dtype=torch.float32)\n","        else:\n","            area = (boxes[:,3]-boxes[:,1])*(boxes[:,2]-boxes[:,0])\n","        labels = torch.ones((num_objs,),dtype = torch.int64)\n","        masks = torch.as_tensor(masks,dtype=torch.uint8)  #these are already binary files\n","        image_id = torch.tensor([idx])\n","        iscrowd = torch.zeros((num_objs,),dtype = torch.int64) #?\n","        target = {}\n","        target[\"boxes\"] = boxes\n","        target[\"labels\"] = labels\n","        target[\"masks\"] = masks\n","        target[\"image_id\"] = image_id\n","        target[\"area\"] = area\n","        target[\"iscrowd\"] = iscrowd\n","        if self.transforms is not None:\n","            img, target = self.transforms(img, target)\n","        return img, target\n","    def __len__(self):\n","        return len(self.imgs)"]},{"cell_type":"code","source":["import torchvision\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n","def get_model_instance_segmentation(num_classes):\n","    # load an instance segmentation model pre-trained pre-trained on COCO\n","    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n","    # get number of input features for the classifier\n","    in_features = model.roi_heads.box_predictor.cls_score.in_features\n","    # replace the pre-trained head with a new one\n","    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","    # now get the number of input features for the mask classifier\n","    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n","    hidden_layer = 256\n","    # and replace the mask predictor with a new one\n","    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n","                                                       hidden_layer,\n","                                                       num_classes)\n","    return model"],"metadata":{"id":"bjL-zBGYFWUn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torchvision.transforms as T\n","from vision.references.detection import engine\n","from vision.references.detection.engine import train_one_epoch, evaluate\n","import utils\n","def get_transform(train):\n","    transforms = []\n","    transforms.append(T.ToTensor())\n","    if train:\n","        transforms.append(T.RandomHorizontalFlip(0.5))\n","    return T.Compose(transforms)"],"metadata":{"id":"T-S5EZ4kFWSK","colab":{"base_uri":"https://localhost:8080/","height":494},"executionInfo":{"status":"error","timestamp":1649678803847,"user_tz":-330,"elapsed":361,"user":{"displayName":"azyo asl","userId":"17530650473661026803"}},"outputId":"f7011337-5777-4d7c-86f1-9e46117ac85d"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-48-19b77c468d2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreferences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreferences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/vision/references/detection/engine.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_rcnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcoco_eval\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCocoEvaluator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcoco_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_coco_api_from_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["!pip install pyttsx3"],"metadata":{"id":"nHmQsWJUFWPl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649678112729,"user_tz":-330,"elapsed":4536,"user":{"displayName":"azyo asl","userId":"17530650473661026803"}},"outputId":"6aa48f24-4de5-4914-b3db-050eb9839a6e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyttsx3\n","  Downloading pyttsx3-2.90-py3-none-any.whl (39 kB)\n","Installing collected packages: pyttsx3\n","Successfully installed pyttsx3-2.90\n"]}]},{"cell_type":"code","source":["!git clone https://github.com/pytorch/vision"],"metadata":{"id":"1ZT2IlFXFokp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649678434370,"user_tz":-330,"elapsed":27141,"user":{"displayName":"azyo asl","userId":"17530650473661026803"}},"outputId":"c50f9432-0882-4b6d-f145-6a2ef8865fb6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'vision'...\n","remote: Enumerating objects: 119020, done.\u001b[K\n","remote: Counting objects: 100% (11726/11726), done.\u001b[K\n","remote: Compressing objects: 100% (1002/1002), done.\u001b[K\n","remote: Total 119020 (delta 10796), reused 11489 (delta 10666), pack-reused 107294\n","Receiving objects: 100% (119020/119020), 230.67 MiB | 31.18 MiB/s, done.\n","Resolving deltas: 100% (103450/103450), done.\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"nS2cfXeQFohv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install py7zr\n","import keras\n","import numpy as np\n","import pandas as pd\n","import os\n","from PIL import Image\n","from shutil import rmtree\n","from re import findall\n","import py7zr\n","from pathlib import Path"],"metadata":{"id":"czuXfT2rFofB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649675358981,"user_tz":-330,"elapsed":3280,"user":{"displayName":"azyo asl","userId":"17530650473661026803"}},"outputId":"c015d19b-5486-48ff-b109-f2fda0fc7fff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: py7zr in /usr/local/lib/python3.7/dist-packages (0.18.3)\n","Requirement already satisfied: pybcj>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from py7zr) (0.5.2)\n","Requirement already satisfied: brotli>=1.0.9 in /usr/local/lib/python3.7/dist-packages (from py7zr) (1.0.9)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from py7zr) (4.11.3)\n","Requirement already satisfied: pycryptodomex>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from py7zr) (3.14.1)\n","Requirement already satisfied: zipfile-deflate64>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from py7zr) (0.2.0)\n","Requirement already satisfied: pyzstd>=0.14.4 in /usr/local/lib/python3.7/dist-packages (from py7zr) (0.15.2)\n","Requirement already satisfied: pyppmd<0.19.0,>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from py7zr) (0.18.2)\n","Requirement already satisfied: multivolumefile>=0.2.3 in /usr/local/lib/python3.7/dist-packages (from py7zr) (0.2.3)\n","Requirement already satisfied: texttable in /usr/local/lib/python3.7/dist-packages (from py7zr) (1.6.4)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->py7zr) (3.7.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->py7zr) (3.10.0.2)\n"]}]},{"cell_type":"code","source":["make_dir = Path('/content/egohands')\n","path_to_source_folder = Path('/content/drive/MyDrive/handmask/egohands_kitti_formatted.7z')\n","if make_dir.exists(): rmtree(make_dir)\n","make_dir.mkdir()\n","\n","if findall('.7z', str(path_to_source_folder)):\n","    with py7zr.SevenZipFile(path_to_source_folder, mode='r') as z:\n","        z.extractall('/content/egohands')\n","    path_to_source_folder = Path('/content/egohands/egohands_kitti_formatted')\n","elif findall('.zip', str(path_to_source_folder)):\n","    !unzip $path_to_source_folder -d /content/ASLdataset\n","    path_to_source_folder = Path('/content/egohands/egohands_kitti_formatted')\n","else:\n","    path_to_source_folder = Path(path_to_source_folder)"],"metadata":{"id":"2lkPVb1vFocV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["imgs = list(os.listdir(os.path.join(path_to_source_folder,\"images\")))\n","boxes = list(os.listdir(os.path.join(path_to_source_folder,\"labels\")))"],"metadata":{"id":"RGryffDFFWM_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab.patches import cv2_imshow\n","import cv2\n","image = cv2.imread('/content/egohands/egohands_kitti_formatted/images/CARDS_COURTYARD_B_T_frame_0011.jpg')\n","cv2_imshow(image)\n","cv2.rectangle(image,(647,453),(825,552),(0,255, 0),2)\n","cv2_imshow(image)\n"],"metadata":{"id":"7HwfP8nMTUU6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def txtToArray(path):\n","    f = open(path)\n","    tokens=f.read().split()\n","    \n","    print(tokens)\n","    for i in range(0,len(tokens)): tokens[i]=tokens[i].split(',')\n","    x_min = tokens[4]\n","    y_min = tokens[5]\n","    x_max = tokens[6]\n","    y_max = tokens[7]\n","    print(x_min,y_min,x_max,y_max)\n","    if 'hand' in tokens:\n","        print('found')\n","    \n","    print(tokens) \n","    A=np.array(tokens, dtype=np.int64)\n","    \n","    return(A)\n","rect = txtToArray('/content/egohands/egohands_kitti_formatted/labels/CARDS_COURTYARD_B_T_frame_0011.txt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"id-Im8yuTUNt","executionInfo":{"status":"ok","timestamp":1649676398210,"user_tz":-330,"elapsed":375,"user":{"displayName":"azyo asl","userId":"17530650473661026803"}},"outputId":"82118fd2-20c7-4b48-9e3e-a1366a93a61d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['647', '453', '825', '552', '515', '431', '623', '544']\n","['515'] ['431'] ['623'] ['544']\n","[['647'], ['453'], ['825'], ['552'], ['515'], ['431'], ['623'], ['544']]\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"3ymcLxUJTULm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"DUjO_J6EFpLN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["all_img_path = list((Path('/content/egohands/egohands_kitti_formatted/images')).glob(\"*\"))"],"metadata":{"id":"lxI7eEE_LQEQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image = str(all_img_path[0])\n","print(image[:-3]+'txt')\n","print(image[:43]+'labels'+image[49:-3]+'txt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BNEUC7iDLTuh","executionInfo":{"status":"ok","timestamp":1649506334321,"user_tz":-330,"elapsed":397,"user":{"displayName":"azyo asl","userId":"17530650473661026803"}},"outputId":"fbd79f0e-b87b-4e58-a1a8-edaee198c0d2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/egohands/egohands_kitti_formatted/images/PUZZLE_LIVINGROOM_B_T_frame_1379.txt\n","/content/egohands/egohands_kitti_formatted/labels/PUZZLE_LIVINGROOM_B_T_frame_1379.txt\n"]}]},{"cell_type":"code","source":["def txtToArray(path):\n","    print(path)\n","    f = open(path)\n","    tokens=f.read().split()\n","    print(tokens)\n","    tokens = [i for i in tokens if i != '0' and i!='hand']\n","    print(tokens)\n","    for i in range(0,len(tokens)): tokens[i]=tokens[i].split(',')\n","    A=np.array(tokens, dtype=np.int)\n","    print(A)\n","    return(A)\n","def __getitem__(idx,all_img_path):\n","    pic_count=idx\n","    i = 1+int(idx/100)\n","    j = 1+(idx%100)\n","    # imgStr = \"Image\"+str(i)+\"_\"+str(j)+\".jpg\"\n","    # boxStr = \"Box\"+str(i)+\"_\"+str(j)+\".txt\"\n","    # img_path = os.path.join('', \"DATA_IMAGES\",imgStr)\n","    # box_path = os.path.join('',\"DATA_BOXES\",boxStr)\n","    img_path = str(all_img_path[pic_count])\n","    box_path = str(str(img_path[:43])+'labels'+str(img_path[49:-3])+'txt')\n","    i+=1\n","    print(img_path)\n","    print(box_path)\n","    box_array = txtToArray(box_path)\n","    im = Image.open(img_path).convert(\"RGB\")\n","    boxes = []\n","    for k in range(len(box_array)):  #get the bounding boxes for all hands in image with xmin,ymin,xmax,ymax coords\n","        if box_array[k,2] != 0:\n","            xmin = box_array[k,0]\n","            ymin = box_array[k,1]\n","            xmax = xmin+box_array[k,2]\n","            ymax = ymin + box_array[k,3]\n","            boxes.append([xmin,ymin,xmax,ymax])\n","    if len(boxes)==0:\n","        idx = (idx+1)%4800\n","        return __getitem__(idx)\n","    big_box = [1199,1199,0,0]         #big box is the Region of Interest, essentially a cropping of the image\n","    for k in range(len(boxes)):\n","        if boxes[k][0] < big_box[0]:\n","            big_box[0] = boxes[k][0]\n","        if boxes[k][1] < big_box[1]:\n","            big_box[1] = boxes[k][1]\n","        if boxes[k][2] > big_box[2]:\n","            big_box[2] = boxes[k][2]\n","        if boxes[k][3] > big_box[3]:\n","            big_box[3] = boxes[k][3]\n","    im_array = np.asarray(im)\n","    ROI = im_array[big_box[1]:big_box[3],big_box[0]:big_box[2],:]          #a crop of the image\n","    ROI_Image = Image.fromarray(ROI, 'RGB')\n","    ROI_Resize = ROI_Image.resize((32,32))\n","    ROI_npArr = np.asarray(ROI_Resize)\n","    #Image_Resize = np.asarray(im.resize((64,64)))\n","    label = 0\n","    if i>0 and i<13:\n","        label = 0    #cards\n","    if i>12 and i<25:\n","        label = 1    #chess\n","    if i>24 and i<37:\n","        label = 2    #jenga\n","    if i>36 and i<49:\n","        label = 3   #puzzle\n","    return ROI_npArr, label"],"metadata":{"id":"Dn1nxU5Gdjs3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train = []\n","Y_train = []\n","X_test = []\n","Y_test = []\n","all_img_path = list((Path('/content/egohands/egohands_kitti_formatted/images')).glob(\"*\"))\n","print(len(all_img_path))\n","for idx in range(len(all_img_path)):\n","    i = 1+int(idx/100)  #video \n","    j = 1+(idx%100)\n","    ROI, activity = __getitem__(idx,all_img_path)\n","    #lets take first 11 videos as training for each activity\n","    toTrain = (i-1)%12\n","    if (toTrain >= 0) and (toTrain <= 10):\n","        X_train.append(ROI)\n","        Y_train.append(activity)\n","    else:\n","        X_test.append(ROI)\n","        Y_test.append(activity)"],"metadata":{"id":"bYGcTfF9FpGG","colab":{"base_uri":"https://localhost:8080/","height":827},"executionInfo":{"status":"error","timestamp":1649680001657,"user_tz":-330,"elapsed":8,"user":{"displayName":"azyo asl","userId":"17530650473661026803"}},"outputId":"3f64f20a-bd25-4dd0-b0b0-ff5708891c9a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["4800\n","/content/egohands/egohands_kitti_formatted/images/PUZZLE_LIVINGROOM_B_T_frame_1379.jpg\n","/content/egohands/egohands_kitti_formatted/labels/PUZZLE_LIVINGROOM_B_T_frame_1379.txt\n","/content/egohands/egohands_kitti_formatted/labels/PUZZLE_LIVINGROOM_B_T_frame_1379.txt\n","['hand', '0', '0', '0', '1', '515', '479', '717', '0', '0', '0', '0', '0', '0', '0', '0', 'hand', '0', '0', '0', '540', '535', '1167', '716', '0', '0', '0', '0', '0', '0', '0', '0', 'hand', '0', '0', '0', '511', '181', '736', '308', '0', '0', '0', '0', '0', '0', '0', '0', 'hand', '0', '0', '0', '288', '146', '495', '297', '0', '0', '0', '0', '0', '0', '0', '0']\n","['1', '515', '479', '717', '540', '535', '1167', '716', '511', '181', '736', '308', '288', '146', '495', '297']\n","[[   1]\n"," [ 515]\n"," [ 479]\n"," [ 717]\n"," [ 540]\n"," [ 535]\n"," [1167]\n"," [ 716]\n"," [ 511]\n"," [ 181]\n"," [ 736]\n"," [ 308]\n"," [ 288]\n"," [ 146]\n"," [ 495]\n"," [ 297]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  if __name__ == '__main__':\n"]},{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-69-81ace854caab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#video\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mROI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_img_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;31m#lets take first 11 videos as training for each activity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtoTrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-68-3bfc8735ae42>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(idx, all_img_path)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m#get the bounding boxes for all hands in image with xmin,ymin,xmax,ymax coords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mbox_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mxmin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbox_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mymin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbox_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for axis 1 with size 1"]}]},{"cell_type":"code","source":["X_tr = np.array(X_train)\n","Y_tr = np.array(Y_train)\n","X_te = np.array(X_test)\n","Y_te = np.array(Y_test)"],"metadata":{"id":"JuCK2sRBFpDQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras.utils import to_categorical\n","Y_tr = to_categorical(Y_tr)\n","Y_te = to_categorical(Y_te)"],"metadata":{"id":"XWgKtF-iFpAU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","plt.imshow(X_tr[1501])\n","plt.show()"],"metadata":{"id":"xWyeOC1pFo9f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras import optimizers\n","from keras.models import Sequential\n","from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D,Dropout,Activation\n","model = Sequential()"],"metadata":{"id":"R5A_thoGFy7Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.add(Conv2D(128,kernel_size=3,activation='relu',input_shape=(32,32,3)))\n","model.add(MaxPooling2D(pool_size=2))\n","model.add(Conv2D(64,kernel_size=3,activation='relu'))\n","model.add(MaxPooling2D(pool_size=2))\n","model.add(Flatten())\n","model.add(Dense(32,activation='relu'))\n","model.add(Dense(4 ,activation='softmax'))"],"metadata":{"id":"6cvjv57dFy4p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sgd = optimizers.SGD(lr=0.00001, decay=1e-6, momentum=0.9, nesterov=True)\n","model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics = ['accuracy'])"],"metadata":{"id":"8gNVlGiwFy2G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.summary()"],"metadata":{"id":"ItVKBnqIFyzY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.fit(X_tr,Y_tr,validation_data=(X_te,Y_te),epochs=30,shuffle=True)"],"metadata":{"id":"uNhfl4BjFywr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["preds = model.predict(X_te)\n","counter = 0\n","for i in range(len(preds)):\n","    preds[i] = np.round(preds[i],0)\n","    if np.array_equal(Y_te[i],preds[i]):\n","        counter = counter +1"],"metadata":{"id":"eLY0RaOOF2_5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["video_scores = [0,0,0,0]\n","for i in range(len(preds)):\n","    video = int(i/100)\n","    if np.array_equal(Y_te[i],preds[i]):\n","        video_scores[video] = video_scores[video]+1\n","print(video_scores)    #this gives amount of frames correctly classified in each of the 4 videos (cards,chess,jenga,puzzle)"],"metadata":{"id":"MLdlc9ZDF29Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"4NSbHdFCF27D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["box_path = '/content/CARDS_COURTYARD_T_B_frame_1432.txt'"],"metadata":{"id":"EV4urASzF24V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def txtToArray(path):\n","    print(path)\n","    f = open(path)\n","    tokens=f.read().split()\n","    print(tokens)\n","    # tokens = [i for i in tokens if i != '0' and i!='hand']\n","    j=0\n","    print(len(tokens))\n","    ftoken = []\n","    while j< len(tokens):\n","        if tokens[j]!='0' and tokens[j]!='hand':\n","            ftoken.extend([tokens[j],tokens[j+1],tokens[j+2],tokens[j+3]])\n","            j=j+4\n","        else:\n","            j+=1\n","\n","    print(ftoken)\n","\n","    # while j<=(len(tokens)):\n","    #     print('j=',j)\n","    #     print('token',tokens[j])\n","    #     if tokens[j]!='hand':\n","    #         if int(tokens[j])!=0:\n","    #             print(tokens)\n","    #             j+=3\n","    #     else:\n","    #         tokens.remove(tokens[j])\n","    #         print(tokens)\n","    #         j+=1\n","    #     print('j=',j)\n","    tokens = splitlist(tokens)\n","    print(tokens)\n","    # for i in range(0,len(tokens)): tokens[i]=tokens[i].split(',')\n","    print(tokens)\n","    A=np.array(tokens, dtype=np.int)\n","    print(A)\n","    return(A)"],"metadata":{"id":"bHoGa81lF21s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["box_array = txtToArray(box_path)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"NGTeXzA2ftCF","executionInfo":{"status":"error","timestamp":1649746782298,"user_tz":-330,"elapsed":3,"user":{"displayName":"azyo asl","userId":"17530650473661026803"}},"outputId":"3fdafd2a-ee77-4fb1-fe55-f9e3031d3b20"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/CARDS_COURTYARD_T_B_frame_1432.txt\n","['hand', '0', '0', '0', '200', '594', '424', '717', '0', '0', '0', '0', '0', '0', '0', '0', 'hand', '0', '0', '0', '567', '619', '770', '718', '0', '0', '0', '0', '0', '0', '0', '0', 'hand', '0', '0', '0', '568', '308', '714', '399', '0', '0', '0', '0', '0', '0', '0', '0', 'hand', '0', '0', '0', '342', '325', '487', '478', '0', '0', '0', '0', '0', '0', '0', '0']\n","64\n","1\n","[]\n","2\n","[]\n","3\n","[]\n","4\n","[]\n","8\n","['200', '594', '424', '717']\n","9\n","['200', '594', '424', '717']\n","10\n","['200', '594', '424', '717']\n","11\n","['200', '594', '424', '717']\n","12\n","['200', '594', '424', '717']\n","13\n","['200', '594', '424', '717']\n","14\n","['200', '594', '424', '717']\n","15\n","['200', '594', '424', '717']\n","16\n","['200', '594', '424', '717']\n","17\n","['200', '594', '424', '717']\n","18\n","['200', '594', '424', '717']\n","19\n","['200', '594', '424', '717']\n","20\n","['200', '594', '424', '717']\n","24\n","['200', '594', '424', '717', '567', '619', '770', '718']\n","25\n","['200', '594', '424', '717', '567', '619', '770', '718']\n","26\n","['200', '594', '424', '717', '567', '619', '770', '718']\n","27\n","['200', '594', '424', '717', '567', '619', '770', '718']\n","28\n","['200', '594', '424', '717', '567', '619', '770', '718']\n","29\n","['200', '594', '424', '717', '567', '619', '770', '718']\n","30\n","['200', '594', '424', '717', '567', '619', '770', '718']\n","31\n","['200', '594', '424', '717', '567', '619', '770', '718']\n","32\n","['200', '594', '424', '717', '567', '619', '770', '718']\n","33\n","['200', '594', '424', '717', '567', '619', '770', '718']\n","34\n","['200', '594', '424', '717', '567', '619', '770', '718']\n","35\n","['200', '594', '424', '717', '567', '619', '770', '718']\n","36\n","['200', '594', '424', '717', '567', '619', '770', '718']\n","40\n","['200', '594', '424', '717', '567', '619', '770', '718', '568', '308', '714', '399']\n","41\n","['200', '594', '424', '717', '567', '619', '770', '718', '568', '308', '714', '399']\n","42\n","['200', '594', '424', '717', '567', '619', '770', '718', '568', '308', '714', '399']\n","43\n","['200', '594', '424', '717', '567', '619', '770', '718', '568', '308', '714', '399']\n","44\n","['200', '594', '424', '717', '567', '619', '770', '718', '568', '308', '714', '399']\n","45\n","['200', '594', '424', '717', '567', '619', '770', '718', '568', '308', '714', '399']\n","46\n","['200', '594', '424', '717', '567', '619', '770', '718', '568', '308', '714', '399']\n","47\n","['200', '594', '424', '717', '567', '619', '770', '718', '568', '308', '714', '399']\n","48\n","['200', '594', '424', '717', '567', '619', '770', '718', '568', '308', '714', '399']\n","49\n","['200', '594', '424', '717', '567', '619', '770', '718', '568', '308', '714', '399']\n","50\n","['200', '594', '424', '717', '567', '619', '770', '718', '568', '308', '714', '399']\n","51\n","['200', '594', '424', '717', '567', '619', '770', '718', '568', '308', '714', '399']\n","52\n","['200', '594', '424', '717', '567', '619', '770', '718', '568', '308', '714', '399']\n","56\n","['200', '594', '424', '717', '567', '619', '770', '718', '568', '308', '714', '399', '342', '325', '487', '478']\n","57\n","['200', '594', '424', '717', '567', '619', '770', '718', '568', '308', '714', '399', '342', '325', '487', '478']\n","58\n","['200', '594', '424', '717', '567', '619', '770', '718', '568', '308', '714', '399', '342', '325', '487', '478']\n","59\n","['200', '594', '424', '717', '567', '619', '770', '718', '568', '308', '714', '399', '342', '325', '487', '478']\n","60\n","['200', '594', '424', '717', '567', '619', '770', '718', '568', '308', '714', '399', '342', '325', '487', '478']\n","61\n","['200', '594', '424', '717', '567', '619', '770', '718', '568', '308', '714', '399', '342', '325', '487', '478']\n","62\n","['200', '594', '424', '717', '567', '619', '770', '718', '568', '308', '714', '399', '342', '325', '487', '478']\n","63\n","['200', '594', '424', '717', '567', '619', '770', '718', '568', '308', '714', '399', '342', '325', '487', '478']\n","64\n","['200', '594', '424', '717', '567', '619', '770', '718', '568', '308', '714', '399', '342', '325', '487', '478']\n","['200', '594', '424', '717', '567', '619', '770', '718', '568', '308', '714', '399', '342', '325', '487', '478']\n"]},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-34-afd4be439ae6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbox_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtxtToArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-33-14f57b42db7a>\u001b[0m in \u001b[0;36mtxtToArray\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m#         j+=1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m#     print('j=',j)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplitlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# for i in range(0,len(tokens)): tokens[i]=tokens[i].split(',')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'splitlist' is not defined"]}]},{"cell_type":"code","source":["char c ='A'\n","long l = c"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":130},"id":"EC4zbN7cfvwl","executionInfo":{"status":"error","timestamp":1649767340579,"user_tz":-330,"elapsed":381,"user":{"displayName":"azyo asl","userId":"17530650473661026803"}},"outputId":"7dca217a-ab60-4fc8-e9c9-f0e50dd95974"},"execution_count":null,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-9e030c1006cd>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    char c ='A'\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"cell_type":"code","source":["!pip install pytorch torchvision cudatoolkit==10.0\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iJZ-katry9Rc","executionInfo":{"status":"ok","timestamp":1649849389024,"user_tz":-330,"elapsed":1523,"user":{"displayName":"azyo asl","userId":"17530650473661026803"}},"outputId":"81bdec37-491f-4005-caa2-f9acccbac089"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pytorch\n","  Downloading pytorch-1.0.2.tar.gz (689 bytes)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.11.1+cu111)\n","\u001b[31mERROR: Could not find a version that satisfies the requirement cudatoolkit==10.0 (from versions: none)\u001b[0m\n","\u001b[31mERROR: No matching distribution found for cudatoolkit==10.0\u001b[0m\n"]}]},{"cell_type":"code","source":["def Preprocessimage(new_image_path)\n","    im = Image.open(new_image_path).convert(\"RGB\")\n","    im_array = np.asarray(im)\n","    ROI_Image = Image.fromarray(im_array, 'RGB')\n","    ROI_Resize = ROI_Image.resize((32,32))\n","    ROI_npArr = np.asarray(ROI_Resize)\n","    return ROI_npArr"],"metadata":{"id":"AxNLm-6Fq4IV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_te = np.array(X_test)"],"metadata":{"id":"KSk8zK1E_Gs3"},"execution_count":null,"outputs":[]}]}